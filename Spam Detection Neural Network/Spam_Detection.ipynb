{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "113f2957-f1a2-4029-9889-128e86d109a9",
   "metadata": {},
   "source": [
    "Subhan Ahmad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0d821-3fc3-464d-885c-4db3979b2f9b",
   "metadata": {},
   "source": [
    "# Spam Classification # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baec2d8-d3dc-4d94-afd2-c36582cabd8d",
   "metadata": {},
   "source": [
    "The purpose of this project is to train a neural network using fine tuning with pytorch to classify whether given text is spam or not. I'm going to use existing dataset \"sms spam\" from [Hugging Face](https://huggingface.co/datasets/sms_spam#discussion-of-biases) to train my neural network model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777da1dc-e511-481b-9817-0a4f14e43057",
   "metadata": {},
   "source": [
    "Resources obtained from :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6833f4-7eb9-4103-996b-05e9801383ca",
   "metadata": {},
   "source": [
    "[Professor Andrew Leahy's Github](https://github.com/aleahy-work/CS-STAT323-W24)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82943cc4-c8e6-4b2c-83a2-2859f70f64ed",
   "metadata": {},
   "source": [
    "[machine-learning-book](https://github.com/rasbt/machine-learning-book)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235f1c8b-e09d-4297-a4be-55e29ebd15e0",
   "metadata": {},
   "source": [
    "# Loading Data # "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8951d5e-44d5-4460-8394-0ecf9f40770f",
   "metadata": {},
   "source": [
    "I'm importing the load dataset function to load the\"sms_spam\" data from huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4ef0cd95-6001-473d-ae75-89fb67866e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9744f627-1007-48d7-a026-770f55a0d9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"sms_spam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "19a28342-5b9b-485e-8e15-ad1794f026ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 5574\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76d7049-0271-4ca4-a996-131012b4f7f5",
   "metadata": {},
   "source": [
    "The dataset consists of two features, sms and label containing the text and corresponding label. The label 0 indicates that the text is not spam whereas 1 indicate that the text is spam"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9beb92-73ab-48b7-8cf2-1b31a8e66886",
   "metadata": {},
   "source": [
    "# Spliting the data into train and validation set #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e6e6ce-c895-4aae-9a3e-9be725506c83",
   "metadata": {},
   "source": [
    "I'm using the train_test_split function to create a train and validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "47af9787-fa4a-4034-a096-25b145fb867a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_valid = dataset['train'].train_test_split(test_size=0.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "1b93626c-6269-4cdb-b620-ae3e65e65a9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 5016\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sms', 'label'],\n",
       "        num_rows: 558\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "034948e9-2901-4329-904c-c22eb462b373",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': ['sms', 'label'], 'test': ['sms', 'label']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_valid.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e0ea32b9-3a63-48af-8f0a-0a4c4eba8cd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mytrain = train_valid['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6875fc48-e9e0-4c18-9d02-f51d4215e8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "myvalid = train_valid['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d7e2a4-651c-42f7-bacd-4af86234abf7",
   "metadata": {},
   "source": [
    "# Tokenizating #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaed717-51d0-456a-a2b7-52f92c079b46",
   "metadata": {},
   "source": [
    "The DistilBertTokenizerFast is a fast tokenizer optimized for usage during training and inference. By loading the pre-trained tokenizer, you can tokenize text data efficiently, preparing it for input into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "7aeb8942-3514-48eb-911c-900c56026712",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ec3b5d-7b71-4715-a4fe-4634ffc76f05",
   "metadata": {},
   "source": [
    "The tokenize_function takes a text from \"sms\" column and converts it into tokens so the text can be computed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bd1df236-a3c6-40f0-8f94-74baba4fd775",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"sms\"], truncation=True, padding='max_length')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef61c7-4f92-49ea-8c4d-1f20dfbc5cf4",
   "metadata": {},
   "source": [
    "map function applies the tokenize_function to the training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "836428bf-37a1-462d-9358-7191af15e58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokened = mytrain.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d5fbcd3-3c3a-4303-bdfc-e4954cca75a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21aa7c33d0fa488ea8e9a19958183692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/558 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_tokened = myvalid.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d57a12-e032-42d3-94ff-9a1038b3188e",
   "metadata": {},
   "source": [
    "# Fine Tuning #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "679d63c5-6df7-4a8d-a1f4-449a8b8dc3f2",
   "metadata": {},
   "source": [
    "The DistilBertForSequenceClassification class is part of the Hugging Face transformers library, specifically designed for sequence classification tasks using DistilBERT. DistilBERT, a distilled version of the BERT (Bidirectional Encoder Representations from Transformers) model, is a model for natural language understanding tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7397cdcd-57d1-44b8-94ea-4d37b7be8af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0aceca-2be5-4aaa-948d-afb22325a1ca",
   "metadata": {},
   "source": [
    "We want perform out computations on a GPU if it is available and the number of epochs for our model are 3. The pre-trained DistilBERT model for sequence classification we are using is the 'distilbert-base-uncased' configuration. This configuration represents a DistilBERT model trained on uncased English text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a6680b97-ed3c-483c-b524-29286509d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_EPOCHS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "197cb347-464b-4488-8392-876a67285ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "model.to(DEVICE)\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b861f5e-6011-404e-a9f6-316e6c7d5ca0",
   "metadata": {},
   "source": [
    "By calling list_metrics(), we obtain a list of strings representing the names of the available metrics. These metrics can then be used to evaluate the performance of models trained on specific datasets or tasks. The compute_metrics function is defined to compute the evaluation metric based on the predictions and labels obtained during evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3409c38a-3dfe-4b72-9d2c-2c3683c495da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import list_metrics\n",
    "metrics_list = list_metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9275bdcf-ed2d-44bb-961d-9661dc33d143",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_metric\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "metric = load_metric(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred \n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(\n",
    "               predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a42b92-77b4-41d4-b048-154f262c3f3a",
   "metadata": {},
   "source": [
    "training_arg uses TrainingArguments class provided by the Hugging Face transformers library. The directory where the model checkpoints and output will be saved is results, the whole model will be trained for 3 epochs, the batch size is 16, the directory where where logs such as training metrics and evaluation results will be saved is set to './logs', training logs are printed every 10 steps and evaluation_strategy=\"epoch\" specifies to perform evaluation at the end of each epoch. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2138acd6-1a8e-4f59-ac78-7196cb5bf87b",
   "metadata": {},
   "source": [
    "trainer sets up all the necessary components for training the model, including the model itself, training and evaluation datasets, optimizer, training arguments, and evaluation metrics computation function. After setting up the Trainer, the training process can be initiated using the trainer.train() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c4c0dfec-3048-44b2-9bcf-d28321c81fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optim = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results', \n",
    "    num_train_epochs=3,     \n",
    "    per_device_train_batch_size=16, \n",
    "    per_device_eval_batch_size=16,   \n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"epoch\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    compute_metrics=compute_metrics,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tokened,\n",
    "    eval_dataset=valid_tokened,\n",
    "    optimizers=(optim, None) \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0346e616-af89-442f-b9fa-26f00c24d918",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='942' max='942' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [942/942 15:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.050400</td>\n",
       "      <td>0.061099</td>\n",
       "      <td>0.980287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.100197</td>\n",
       "      <td>0.982079</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000300</td>\n",
       "      <td>0.097228</td>\n",
       "      <td>0.985663</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checkpoint destination directory ./results/checkpoint-500 already exists and is non-empty. Saving will proceed but saved results may be invalid.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=942, training_loss=0.02862816490887935, metrics={'train_runtime': 930.485, 'train_samples_per_second': 16.172, 'train_steps_per_second': 1.012, 'total_flos': 1993369414975488.0, 'train_loss': 0.02862816490887935, 'epoch': 3.0})"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "23682120-791a-4986-a5a1-54eaa5235fca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='35' max='35' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [35/35 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.09722808003425598,\n",
       " 'eval_accuracy': 0.985663082437276,\n",
       " 'eval_runtime': 10.8146,\n",
       " 'eval_samples_per_second': 51.597,\n",
       " 'eval_steps_per_second': 3.236,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f82fcf-0603-4ae5-a833-849c506b2910",
   "metadata": {},
   "source": [
    "The model has an accuracy  of 0.9856"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "bf2f4007-612e-4ec3-8245-dcdf59557a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('sms_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed72cf4-89a3-4ee5-9c0d-3bf7031a5264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
